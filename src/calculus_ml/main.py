import os
import numpy as np
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import track
from rich.tree import Tree
from rich import print as rprint

# Import core functionality
from .core.vector import (
    add_vectors,
    dot_product,
    vector_norm,
    create_vectors,
    vector_operations,
    unit_vector_and_angle,
    vector_projection
)
from .core.cost_function import (
    compute_cost, compute_cost_2d,
    generate_house_data, generate_house_data_2d
)
from .core.gradient_descent import (
    gradient_descent, gradient_descent_2d,
    compute_gradient, compute_gradient_2d
)

# Import visualization
from .visualization.cost_plot import (
    plot_cost_3d,
    plot_cost_contour,
    plot_linear_regression_fit
)
from .visualization.gradient_plot import plot_gradient_descent, plot_gradient_steps

# Import logistic regression
from .core.logistic_regression import (
    generate_admission_data,
    gradient_descent_logistic,
    compute_cost_logistic
)
from .visualization.logistic_plot import (
    plot_decision_boundary,
    plot_cost_surface_logistic,
    plot_gradient_descent_logistic
)

# Initialize rich console
console = Console()

# Dictionary qu·∫£n l√Ω h√¨nh ·∫£nh
IMAGES = {
    "Cost Function": {
        "cost_function_3d.png": "B·ªÅ m·∫∑t cost function trong kh√¥ng gian 3D",
        "cost_function_contour.png": "ƒê∆∞·ªùng ƒë·ªìng m·ª©c c·ªßa cost function",
        "logistic_cost_3d.png": "B·ªÅ m·∫∑t cost function cho logistic regression"
    },
    "Gradient Descent": {
        "gradient_descent_3d.png": "Qu√° tr√¨nh gradient descent tr√™n b·ªÅ m·∫∑t cost 3D",
        "gradient_descent_contour.png": "Qu√° tr√¨nh gradient descent tr√™n contour",
        "gradient_descent_steps.png": "C√°c b∆∞·ªõc c·ªßa gradient descent tr√™n d·ªØ li·ªáu",
        "cost_history.png": "L·ªãch s·ª≠ cost function qua c√°c iteration",
        "decision_boundary.png": "Decision boundary cho logistic regression",
        "logistic_gradient_descent.png": "Qu√° tr√¨nh gradient descent cho logistic regression"
    }
}

def ensure_images_dir():
    """ƒê·∫£m b·∫£o th∆∞ m·ª•c images t·ªìn t·∫°i"""
    if not os.path.exists('images'):
        os.makedirs('images')

def print_generated_images():
    """In th√¥ng tin v·ªÅ c√°c h√¨nh ·∫£nh ƒë√£ t·∫°o"""
    tree = Tree("üìä H√¨nh ·∫£nh ƒë√£ t·∫°o")
    
    for category, images in IMAGES.items():
        category_tree = tree.add(f"üìÅ {category}")
        for img_name, description in images.items():
            img_path = os.path.join('images', img_name)
            if os.path.exists(img_path):
                size = os.path.getsize(img_path) / 1024  # Convert to KB
                category_tree.add(f"üìÑ {img_name} ({size:.1f}KB) - {description}")
            else:
                category_tree.add(f"‚ùå {img_name} (kh√¥ng t√¨m th·∫•y) - {description}")
    
    console.print("\n")
    console.print(Panel(tree, title="[bold blue]Th√¥ng tin h√¨nh ·∫£nh[/bold blue]"))
    console.print("\n")

def run_vector_examples():
    """Ch·∫°y c√°c v√≠ d·ª• v·ªÅ vector c∆° b·∫£n"""
    console.print("\n[bold cyan]1. Vector Operations[/bold cyan]", justify="center")
    
    # Create sample vectors
    v1, v2 = create_vectors()
    
    # Perform vector operations
    operations = vector_operations(v1, v2)
    unit_angle = unit_vector_and_angle(v1, v2)
    projection = vector_projection(v1, v2)
    
    # Create table for results
    table = Table(title="Vector Operations Results")
    table.add_column("Operation", style="cyan")
    table.add_column("Result", style="green")
    
    table.add_row("Vector 1", str(v1))
    table.add_row("Vector 2", str(v2))
    table.add_row("Sum", str(operations['v_sum']))
    table.add_row("Difference", str(operations['v_diff']))
    table.add_row("Scaled (2*v1)", str(operations['v_scaled']))
    table.add_row("Norm of v1", f"{operations['v1_norm']:.2f}")
    table.add_row("Dot product", str(operations['dot_product']))
    table.add_row("Cross product", str(operations['cross_product']))
    table.add_row("Angle between vectors", f"{unit_angle['angle']:.2f}¬∞")
    table.add_row("Projection norm", f"{projection['projection_norm']:.2f}")
    
    console.print(table)

def run_cost_and_gradient_example():
    """Ch·∫°y v√≠ d·ª• v·ªÅ cost function v√† gradient descent"""
    console.print("\n[bold cyan]2. Cost Function v√† Gradient Descent[/bold cyan]", justify="center")
    
    # Ph·∫ßn 1: V√≠ d·ª• v·ªõi 1 tham s·ªë
    console.print("\n[bold magenta]2.1. V√≠ d·ª• v·ªõi 1 tham s·ªë (k√≠ch th∆∞·ªõc nh√†)[/bold magenta]")
    run_one_feature_example()
    
    # Ph·∫ßn 2: V√≠ d·ª• v·ªõi 2 tham s·ªë
    console.print("\n[bold magenta]2.2. V√≠ d·ª• v·ªõi 2 tham s·ªë (k√≠ch th∆∞·ªõc v√† s·ªë ph√≤ng ng·ªß)[/bold magenta]")
    run_two_features_example()

def run_one_feature_example():
    """Ch·∫°y v√≠ d·ª• v·ªõi 1 tham s·ªë"""
    # T·∫°o d·ªØ li·ªáu m·∫´u v·ªÅ gi√° nh√†
    x_train, y_train = generate_house_data()
    
    # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ v√≠ d·ª•
    console.print(Panel(
        "[bold green]Th√¥ng tin v·ªÅ v√≠ d·ª•:[/bold green]\n"
        "1. D·ªØ li·ªáu m·∫´u:\n"
        "   - x: k√≠ch th∆∞·ªõc nh√† (1000 sqft)\n"
        "   - y: gi√° nh√† (1000s $)\n"
        "   - S·ªë m·∫´u: {len(x_train)}\n\n"
        "2. C√¥ng th·ª©c:\n"
        "   - H√†m d·ª± ƒëo√°n: f(x) = wx + b\n"
        "   - Cost function: J(w,b) = (1/2m) * Œ£(f(x‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ)¬≤\n"
        "   - Gradient:\n"
        "     * ‚àÇJ/‚àÇw = (1/m) * Œ£(f(x‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ) * x‚ÅΩ‚Å±‚Åæ\n"
        "     * ‚àÇJ/‚àÇb = (1/m) * Œ£(f(x‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ)\n"
        "   - C·∫≠p nh·∫≠t tham s·ªë:\n"
        "     * w = w - Œ± * ‚àÇJ/‚àÇw\n"
        "     * b = b - Œ± * ‚àÇJ/‚àÇb",
        title="Example Overview",
        border_style="cyan"
    ))
    
    # Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u
    console.print(Panel(
        f"[bold green]D·ªØ li·ªáu training:[/bold green]\n"
        f"K√≠ch th∆∞·ªõc nh√†: {x_train}\n"
        f"Gi√° nh√†: {y_train}",
        title="Training Data",
        border_style="cyan"
    ))
    
    # T√≠nh cost function t·∫°i m·ªôt ƒëi·ªÉm
    w_example, b_example = 180, 150  # Thay ƒë·ªïi ƒëi·ªÉm minh h·ªça
    cost = compute_cost(x_train, y_train, w_example, b_example)
    
    # Debug prints
    console.print(f"\n[bold red]Debug:[/bold red]")
    console.print(f"x_train: {x_train}")
    console.print(f"y_train: {y_train}")
    console.print(f"w_example: {w_example}")
    console.print(f"b_example: {b_example}")
    console.print(f"Actual cost: {cost}")
    
    console.print(Panel(
        f"[bold green]Minh h·ªça Cost Function t·∫°i m·ªôt ƒëi·ªÉm:[/bold green]\n"
        f"Ch·ªçn ƒëi·ªÉm (w={w_example}, b={b_example}) ƒë·ªÉ minh h·ªça c√°ch t√≠nh cost function:\n"
        f"- w = {w_example}: gi√° tƒÉng {w_example}$ cho m·ªói 1000 sqft\n"
        f"- b = {b_example}: gi√° c∆° b·∫£n {b_example}$1000\n"
        f"- Cost = {cost:.2f}: ƒë·ªô l·ªách trung b√¨nh b√¨nh ph∆∞∆°ng c·ªßa d·ª± ƒëo√°n\n\n"
        f"[yellow]Gi·∫£i th√≠ch:[/yellow]\n"
        f"T·∫°i ƒëi·ªÉm n√†y, m√¥ h√¨nh c√≥ m·ªôt s·ªë sai s·ªë trong d·ª± ƒëo√°n:\n"
        f"- Nh√† 1000 sqft: d·ª± ƒëo√°n = {w_example*1 + b_example} = {w_example*1 + b_example:.0f} (th·ª±c t·∫ø: 300)\n"
        f"- Nh√† 2000 sqft: d·ª± ƒëo√°n = {w_example*2 + b_example} = {w_example*2 + b_example:.0f} (th·ª±c t·∫ø: 500)\n"
        f"- Nh√† 3000 sqft: d·ª± ƒëo√°n = {w_example*3 + b_example} = {w_example*3 + b_example:.0f} (th·ª±c t·∫ø: 700)\n"
        f"- Nh√† 4000 sqft: d·ª± ƒëo√°n = {w_example*4 + b_example} = {w_example*4 + b_example:.0f} (th·ª±c t·∫ø: 900)\n"
        f"- Nh√† 5000 sqft: d·ª± ƒëo√°n = {w_example*5 + b_example} = {w_example*5 + b_example:.0f} (th·ª±c t·∫ø: 1100)\n\n"
        f"[yellow]L∆∞u √Ω:[/yellow] ƒê√¢y l√† m·ªôt ƒëi·ªÉm b·∫•t k·ª≥ tr√™n b·ªÅ m·∫∑t cost function.\n"
        f"Trong ph·∫ßn ti·∫øp theo, ch√∫ng ta s·∫Ω t√¨m ƒëi·ªÉm t·ªëi ∆∞u (w*, b*) c√≥ cost th·∫•p nh·∫•t.",
        title="Cost Function Evaluation",
        border_style="cyan"
    ))
    
    # V·∫Ω ƒë·ªì th·ªã cost function
    with console.status("[bold green]T·∫°o ƒë·ªì th·ªã cost function..."):
        plot_cost_3d(x_train, y_train, w_range=(100, 300), b_range=(-200, 200), save_as='cost_function_3d.png')
        plot_cost_contour(x_train, y_train, w_range=(100, 300), b_range=(-200, 200), save_as='cost_function_contour.png')
    
    # Th·ª±c hi·ªán gradient descent
    initial_w, initial_b = 100, 0
    iterations = 1000
    alpha = 0.01
    
    console.print(Panel(
        f"[bold green]Th√¥ng tin gradient descent:[/bold green]\n"
        f"- Learning rate (Œ±): {alpha}\n"
        f"- S·ªë iteration: {iterations}\n"
        f"- Tham s·ªë kh·ªüi t·∫°o: w = {initial_w}, b = {initial_b}",
        title="Gradient Descent Setup",
        border_style="cyan"
    ))
    
    with console.status("[bold green]Running gradient descent..."):
        w_final, b_final, J_hist, p_hist = gradient_descent(
            x_train, y_train, initial_w, initial_b, alpha, iterations, compute_cost)
    
    # T√°ch l·ªãch s·ª≠ tham s·ªë
    w_hist = [p[0] for p in p_hist]
    b_hist = [p[1] for p in p_hist]
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    console.print(Panel(
        f"[bold green]K·∫øt qu·∫£ t·ªëi ∆∞u sau {iterations} iterations:[/bold green]\n"
        f"1. Tham s·ªë t·ªëi ∆∞u (w*, b*):\n"
        f"   w* = {w_final:.2f}\n"
        f"   b* = {b_final:.2f}\n"
        f"2. Ph∆∞∆°ng tr√¨nh h·ªìi quy t·ªëi ∆∞u:\n"
        f"   y = {w_final:.2f}x + {b_final:.2f}\n"
        f"3. Cost t·ªëi ∆∞u: {J_hist[-1]:.4f}\n\n"
        f"[yellow]So s√°nh v·ªõi ƒëi·ªÉm minh h·ªça:[/yellow]\n"
        f"- ƒêi·ªÉm minh h·ªça (w={w_example}, b={b_example}): Cost = {cost:.2f}\n"
        f"- ƒêi·ªÉm t·ªëi ∆∞u (w*={w_final:.2f}, b*={b_final:.2f}): Cost = {J_hist[-1]:.4f}\n"
        f"‚Üí ƒêi·ªÉm t·ªëi ∆∞u c√≥ cost th·∫•p h∆°n, nghƒ©a l√† m√¥ h√¨nh d·ª± ƒëo√°n ch√≠nh x√°c h∆°n.",
        title="Optimization Results",
        border_style="cyan"
    ))
    
    # T·∫°o b·∫£ng theo d√µi cost
    cost_table = Table(title="Theo d√µi Cost qua c√°c Iteration")
    cost_table.add_column("Iteration", style="cyan", justify="right")
    cost_table.add_column("Cost", style="green", justify="right")
    cost_table.add_column("Thay ƒë·ªïi", style="yellow", justify="right")
    
    # Th√™m c√°c m·ªëc quan tr·ªçng
    milestones = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, iterations-1]
    for i in milestones:
        if i < len(J_hist):
            cost = J_hist[i]
            if i > 0:
                change = J_hist[i] - J_hist[i-1]
                change_str = f"{change:+.4f}"
            else:
                change_str = "-"
            cost_table.add_row(
                f"{i:4d}",
                f"{cost:.4f}",
                change_str
            )
    
    console.print(cost_table)
    
    # V·∫Ω ƒë·ªì th·ªã gradient descent
    with console.status("[bold green]T·∫°o ƒë·ªì th·ªã gradient descent..."):
        plot_gradient_descent(x_train, y_train, w_hist, b_hist, J_hist, compute_cost, save_as='gradient_descent_3d.png')
        plot_gradient_steps(x_train, y_train, w_hist, b_hist, compute_cost, save_as='gradient_descent_steps.png')

def run_two_features_example():
    """Ch·∫°y v√≠ d·ª• v·ªõi 2 tham s·ªë"""
    # T·∫°o d·ªØ li·ªáu m·∫´u v·ªÅ gi√° nh√†
    x1_train, x2_train, y_train = generate_house_data_2d()
    
    # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ v√≠ d·ª•
    console.print(Panel(
        "[bold green]Th√¥ng tin v·ªÅ v√≠ d·ª• 2 tham s·ªë:[/bold green]\n"
        "1. D·ªØ li·ªáu m·∫´u:\n"
        "   - x‚ÇÅ: k√≠ch th∆∞·ªõc nh√† (1000 sqft)\n"
        "   - x‚ÇÇ: s·ªë ph√≤ng ng·ªß\n"
        "   - y: gi√° nh√† (1000s $)\n"
        "   - S·ªë m·∫´u: {len(x1_train)}\n\n"
        "2. C√¥ng th·ª©c:\n"
        "   - H√†m d·ª± ƒëo√°n: f(x‚ÇÅ,x‚ÇÇ) = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + b\n"
        "   - Cost function: J(w‚ÇÅ,w‚ÇÇ,b) = (1/2m) * Œ£(f(x‚ÇÅ‚ÅΩ‚Å±‚Åæ,x‚ÇÇ‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ)¬≤\n"
        "   - Gradient:\n"
        "     * ‚àÇJ/‚àÇw‚ÇÅ = (1/m) * Œ£(f(x‚ÇÅ‚ÅΩ‚Å±‚Åæ,x‚ÇÇ‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ) * x‚ÇÅ‚ÅΩ‚Å±‚Åæ\n"
        "     * ‚àÇJ/‚àÇw‚ÇÇ = (1/m) * Œ£(f(x‚ÇÅ‚ÅΩ‚Å±‚Åæ,x‚ÇÇ‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ) * x‚ÇÇ‚ÅΩ‚Å±‚Åæ\n"
        "     * ‚àÇJ/‚àÇb = (1/m) * Œ£(f(x‚ÇÅ‚ÅΩ‚Å±‚Åæ,x‚ÇÇ‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ)",
        title="Example Overview (2 Features)",
        border_style="cyan"
    ))
    
    # Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u
    console.print(Panel(
        f"[bold green]D·ªØ li·ªáu training:[/bold green]\n"
        f"K√≠ch th∆∞·ªõc nh√† (x‚ÇÅ): {x1_train}\n"
        f"S·ªë ph√≤ng ng·ªß (x‚ÇÇ): {x2_train}\n"
        f"Gi√° nh√† (y): {y_train}",
        title="Training Data (2 Features)",
        border_style="cyan"
    ))
    
    # T√≠nh cost function t·∫°i m·ªôt ƒëi·ªÉm
    w1_example, w2_example, b_example = 150, 50, 100
    cost = compute_cost_2d(x1_train, x2_train, y_train, w1_example, w2_example, b_example)
    
    console.print(Panel(
        f"[bold green]Minh h·ªça Cost Function t·∫°i m·ªôt ƒëi·ªÉm:[/bold green]\n"
        f"Ch·ªçn ƒëi·ªÉm (w‚ÇÅ={w1_example}, w‚ÇÇ={w2_example}, b={b_example}) ƒë·ªÉ minh h·ªça:\n"
        f"- w‚ÇÅ = {w1_example}: gi√° tƒÉng {w1_example}$ cho m·ªói 1000 sqft\n"
        f"- w‚ÇÇ = {w2_example}: gi√° tƒÉng {w2_example}$ cho m·ªói ph√≤ng ng·ªß\n"
        f"- b = {b_example}: gi√° c∆° b·∫£n {b_example}$1000\n"
        f"- Cost = {cost:.2f}: ƒë·ªô l·ªách trung b√¨nh b√¨nh ph∆∞∆°ng c·ªßa d·ª± ƒëo√°n\n\n"
        f"[yellow]Gi·∫£i th√≠ch:[/yellow]\n"
        f"T·∫°i ƒëi·ªÉm n√†y, m√¥ h√¨nh d·ª± ƒëo√°n:\n"
        f"- Nh√† 1000sqft, 1 ph√≤ng ng·ªß: {w1_example*1 + w2_example*1 + b_example:.0f} (th·ª±c t·∫ø: 250)\n"
        f"- Nh√† 2000sqft, 1 ph√≤ng ng·ªß: {w1_example*2 + w2_example*1 + b_example:.0f} (th·ª±c t·∫ø: 350)\n"
        f"- Nh√† 2000sqft, 2 ph√≤ng ng·ªß: {w1_example*2 + w2_example*2 + b_example:.0f} (th·ª±c t·∫ø: 400)\n"
        f"- Nh√† 3000sqft, 2 ph√≤ng ng·ªß: {w1_example*3 + w2_example*2 + b_example:.0f} (th·ª±c t·∫ø: 500)\n"
        f"- Nh√† 3000sqft, 3 ph√≤ng ng·ªß: {w1_example*3 + w2_example*3 + b_example:.0f} (th·ª±c t·∫ø: 550)\n"
        f"- Nh√† 4000sqft, 3 ph√≤ng ng·ªß: {w1_example*4 + w2_example*3 + b_example:.0f} (th·ª±c t·∫ø: 650)",
        title="Cost Function Evaluation (2 Features)",
        border_style="cyan"
    ))
    
    # Th·ª±c hi·ªán gradient descent
    initial_w1, initial_w2, initial_b = 100, 0, 0
    iterations = 1000
    alpha = 0.01
    
    console.print(Panel(
        f"[bold green]Th√¥ng tin gradient descent:[/bold green]\n"
        f"- Learning rate (Œ±): {alpha}\n"
        f"- S·ªë iteration: {iterations}\n"
        f"- Tham s·ªë kh·ªüi t·∫°o:\n"
        f"  * w‚ÇÅ = {initial_w1}\n"
        f"  * w‚ÇÇ = {initial_w2}\n"
        f"  * b = {initial_b}",
        title="Gradient Descent Setup (2 Features)",
        border_style="cyan"
    ))
    
    with console.status("[bold green]Running gradient descent..."):
        w1_final, w2_final, b_final, J_hist, p_hist = gradient_descent_2d(
            x1_train, x2_train, y_train,
            initial_w1, initial_w2, initial_b,
            alpha, iterations, compute_cost_2d)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    console.print(Panel(
        f"[bold green]K·∫øt qu·∫£ t·ªëi ∆∞u sau {iterations} iterations:[/bold green]\n"
        f"1. Tham s·ªë t·ªëi ∆∞u (w‚ÇÅ*, w‚ÇÇ*, b*):\n"
        f"   w‚ÇÅ* = {w1_final:.2f}: ·∫£nh h∆∞·ªüng c·ªßa k√≠ch th∆∞·ªõc nh√†\n"
        f"   w‚ÇÇ* = {w2_final:.2f}: ·∫£nh h∆∞·ªüng c·ªßa s·ªë ph√≤ng ng·ªß\n"
        f"   b* = {b_final:.2f}: gi√° c∆° b·∫£n\n"
        f"2. Ph∆∞∆°ng tr√¨nh h·ªìi quy t·ªëi ∆∞u:\n"
        f"   y = {w1_final:.2f}x‚ÇÅ + {w2_final:.2f}x‚ÇÇ + {b_final:.2f}\n"
        f"3. Cost t·ªëi ∆∞u: {J_hist[-1]:.4f}\n\n"
        f"[yellow]So s√°nh v·ªõi ƒëi·ªÉm minh h·ªça:[/yellow]\n"
        f"- ƒêi·ªÉm minh h·ªça (w‚ÇÅ={w1_example}, w‚ÇÇ={w2_example}, b={b_example}): Cost = {cost:.2f}\n"
        f"- ƒêi·ªÉm t·ªëi ∆∞u (w‚ÇÅ*={w1_final:.2f}, w‚ÇÇ*={w2_final:.2f}, b*={b_final:.2f}): Cost = {J_hist[-1]:.4f}\n"
        f"‚Üí ƒêi·ªÉm t·ªëi ∆∞u c√≥ cost th·∫•p h∆°n, nghƒ©a l√† m√¥ h√¨nh d·ª± ƒëo√°n ch√≠nh x√°c h∆°n.",
        title="Optimization Results (2 Features)",
        border_style="cyan"
    ))
    
    # T·∫°o b·∫£ng theo d√µi cost
    cost_table = Table(title="Theo d√µi Cost qua c√°c Iteration (2 Features)")
    cost_table.add_column("Iteration", style="cyan", justify="right")
    cost_table.add_column("Cost", style="green", justify="right")
    cost_table.add_column("Thay ƒë·ªïi", style="yellow", justify="right")
    
    # Th√™m c√°c m·ªëc quan tr·ªçng
    milestones = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, iterations-1]
    for i in milestones:
        if i < len(J_hist):
            cost = J_hist[i]
            if i > 0:
                change = J_hist[i] - J_hist[i-1]
                change_str = f"{change:+.4f}"
            else:
                change_str = "-"
            cost_table.add_row(
                f"{i:4d}",
                f"{cost:.4f}",
                change_str
            )
    
    console.print(cost_table)

def run_logistic_regression_example():
    """Ch·∫°y v√≠ d·ª• v·ªÅ logistic regression"""
    console.print("\n[bold cyan]3. Logistic Regression Example[/bold cyan]", justify="center")
    
    # Hi·ªÉn th·ªã c√¥ng th·ª©c
    console.print(Panel(
        "[bold green]C√¥ng th·ª©c t√≠nh to√°n:[/bold green]\n"
        "1. H√†m sigmoid: g(z) = 1 / (1 + e^(-z))\n"
        "2. H√†m d·ª± ƒëo√°n: h(x) = g(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + b)\n"
        "3. Cost function: J(w‚ÇÅ,w‚ÇÇ,b) = -(1/m) * Œ£[y‚ÅΩ‚Å±‚Åælog(h(x‚ÅΩ‚Å±‚Åæ)) + (1-y‚ÅΩ‚Å±‚Åæ)log(1-h(x‚ÅΩ‚Å±‚Åæ))]\n"
        "4. Gradient:\n"
        "   - ‚àÇJ/‚àÇw‚ÇÅ = (1/m) * Œ£(h(x‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ) * x‚ÇÅ‚ÅΩ‚Å±‚Åæ\n"
        "   - ‚àÇJ/‚àÇw‚ÇÇ = (1/m) * Œ£(h(x‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ) * x‚ÇÇ‚ÅΩ‚Å±‚Åæ\n"
        "   - ‚àÇJ/‚àÇb = (1/m) * Œ£(h(x‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ)",
        title="Logistic Regression Formulas",
        border_style="cyan"
    ))
    
    # T·∫°o d·ªØ li·ªáu m·∫´u
    x1, x2, y = generate_admission_data()
    
    # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ d·ªØ li·ªáu
    console.print(Panel(
        f"[bold green]D·ªØ li·ªáu tuy·ªÉn sinh:[/bold green]\n"
        f"- S·ªë l∆∞·ª£ng m·∫´u: {len(y)}\n"
        f"- S·ªë l∆∞·ª£ng sinh vi√™n ƒë·ªó: {np.sum(y)}\n"
        f"- S·ªë l∆∞·ª£ng sinh vi√™n tr∆∞·ª£t: {len(y) - np.sum(y)}\n"
        f"- ƒêi·ªÉm thi: min={x1.min():.1f}, max={x1.max():.1f}, mean={x1.mean():.1f}\n"
        f"- GPA: min={x2.min():.1f}, max={x2.max():.1f}, mean={x2.mean():.1f}",
        title="Training Data",
        border_style="cyan"
    ))
    
    # Th·ª±c hi·ªán gradient descent
    initial_w1, initial_w2, initial_b = 0, 0, 0
    iterations = 1000
    alpha = 0.01
    
    console.print(Panel(
        f"[bold green]Th√¥ng tin gradient descent:[/bold green]\n"
        f"- Learning rate (Œ±): {alpha}\n"
        f"- S·ªë iteration: {iterations}\n"
        f"- Tham s·ªë kh·ªüi t·∫°o:\n"
        f"  * w‚ÇÅ = {initial_w1}\n"
        f"  * w‚ÇÇ = {initial_w2}\n"
        f"  * b = {initial_b}",
        title="Gradient Descent Setup",
        border_style="cyan"
    ))
    
    with console.status("[bold green]Running gradient descent..."):
        w1_final, w2_final, b_final, J_hist, p_hist = gradient_descent_logistic(
            x1, x2, y, initial_w1, initial_w2, initial_b, alpha, iterations)
    
    # T√°ch l·ªãch s·ª≠ tham s·ªë
    w1_hist = [p[0] for p in p_hist]
    w2_hist = [p[1] for p in p_hist]
    b_hist = [p[2] for p in p_hist]
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    console.print(Panel(
        f"[bold green]K·∫øt qu·∫£ t·ªëi ∆∞u sau {iterations} iterations:[/bold green]\n"
        f"1. Tham s·ªë t·ªëi ∆∞u (w‚ÇÅ*, w‚ÇÇ*, b*):\n"
        f"   w‚ÇÅ* = {w1_final:.4f}: tr·ªçng s·ªë ƒëi·ªÉm thi\n"
        f"   w‚ÇÇ* = {w2_final:.4f}: tr·ªçng s·ªë GPA\n"
        f"   b* = {b_final:.4f}: ƒë·ªô ch·ªách\n"
        f"2. Ph∆∞∆°ng tr√¨nh ph√¢n lo·∫°i:\n"
        f"   P(ƒë·ªó) = g({w1_final:.4f}x‚ÇÅ + {w2_final:.4f}x‚ÇÇ + {b_final:.4f})\n"
        f"3. Cost t·ªëi ∆∞u: {J_hist[-1]:.4f}",
        title="Optimization Results",
        border_style="cyan"
    ))
    
    # T·∫°o b·∫£ng theo d√µi cost
    cost_table = Table(title="Theo d√µi Cost qua c√°c Iteration")
    cost_table.add_column("Iteration", style="cyan", justify="right")
    cost_table.add_column("Cost", style="green", justify="right")
    cost_table.add_column("Thay ƒë·ªïi", style="yellow", justify="right")
    
    # Th√™m c√°c m·ªëc quan tr·ªçng
    milestones = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, iterations-1]
    for i in milestones:
        if i < len(J_hist):
            cost = J_hist[i]
            if i > 0:
                change = J_hist[i] - J_hist[i-1]
                change_str = f"{change:+.4f}"
            else:
                change_str = "-"
            cost_table.add_row(
                f"{i:4d}",
                f"{cost:.4f}",
                change_str
            )
    
    console.print(cost_table)
    
    # V·∫Ω c√°c ƒë·ªì th·ªã
    with console.status("[bold green]T·∫°o ƒë·ªì th·ªã..."):
        plot_decision_boundary(x1, x2, y, w1_final, w2_final, b_final)
        plot_cost_surface_logistic(x1, x2, y, [-1, 1], [-1, 1], b_final)
        plot_gradient_descent_logistic(x1, x2, y, w1_hist, w2_hist, b_hist, J_hist)

def main():
    """H√†m main ch·∫°y t·∫•t c·∫£ c√°c v√≠ d·ª•"""
    console.print(Panel.fit(
        "[bold blue]·ª®ng D·ª•ng Gi·∫£i T√≠ch v√† H·ªçc M√°y[/bold blue]\n"
        "[italic]Minh h·ªça c√°c kh√°i ni·ªám c∆° b·∫£n trong h·ªçc m√°y[/italic]",
        border_style="blue"
    ))
    
    # Ensure images directory exists
    ensure_images_dir()
    
    # Ch·∫°y c√°c v√≠ d·ª•
    for example in track([
        run_vector_examples,
        run_cost_and_gradient_example,
        run_logistic_regression_example
    ], description="Running examples..."):
        example()
    
    # In th√¥ng tin v·ªÅ c√°c h√¨nh ·∫£nh ƒë√£ t·∫°o
    print_generated_images()

if __name__ == "__main__":
    main() 